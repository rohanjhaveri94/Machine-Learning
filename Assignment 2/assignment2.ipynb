{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2:  the perceptron\n",
    "\n",
    "Due date:  Friday 9/21 at 11:59pm\n",
    "\n",
    "Name: Rohan Jhaveri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Datasets\n",
    "\n",
    "In this assignment we will use the following datasets:\n",
    "  * The [Gisette](http://archive.ics.uci.edu/ml/datasets/Gisette) handwritten digit recognition dataset. \n",
    "  * The [QSAR](http://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation) data for predicting the biochemical activity of a molecule.\n",
    "  * The [Heart disease diagnosis](http://archive.ics.uci.edu/ml/datasets/Heart+Disease) dataset.\n",
    "  * For developing your code, you can use one of the scikit-learn datasets, such as the [breast cancer wisconsin dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) and the [make_classification](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification) toy dataset generator.\n",
    "  \n",
    "When writing your notebook, you can assume the datasets are in the same directory as the notebook.  Please keep the same file names as in the UCI repository.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Variants of the perceptron algorithm \n",
    "\n",
    "In this assignment you will work with several variants of the perceptron algorithm:\n",
    "\n",
    "  * The \"vanila\" version of the perceptron algorithm, which was introduced in class.\n",
    "  * The pocket algorithm as described in the slides or page 80 in the book.\n",
    "  * The **adatron** version of the perceptron described next.\n",
    "\n",
    "In each case make sure that your implementation of the classifier **includes a bias term** (in slide set 2 and page 7 in the book you will find guidance on how to add a bias term to an algorithm that is expressed without one).\n",
    "\n",
    "## The adatron \n",
    "\n",
    "Before we get to the adatron, we will derive an alternative form of the perceptron algorithm --- the dual perceptron algorithm.  All we need to look at is the weight update rule:\n",
    "\n",
    "$$\\mathbf{w} \\rightarrow \\mathbf{w} + \\eta y_i \\mathbf{x}_i.$$\n",
    "\n",
    "This is performed whenever example $i$ is misclassified by the current weight vector.  The thing to notice, is that the weight vector is always a weighted combination of the training examples since it is that way to begin with, and each update maintains that property.  So in fact, rather than representing $\\mathbf{w}$ explicitly, all we need to do is to keep track of how much each training example is contributing to the value of the weight vector, i.e. we will express it as:\n",
    "\n",
    "$$\\mathbf{w} = \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i,$$\n",
    "\n",
    "where $\\alpha_i$ are positive numbers that describe the magnitude of the contribution $\\mathbf{x}_i$ is making to the weight vector, and $N$ is the number of training examples.\n",
    "\n",
    "Therefore to initialize $\\mathbf{w}$ to 0, we simply initialize $\\alpha_i = 0$ for $i = 1,\\ldots,N$.  When expressed using the variables $\\alpha_i$, the perceptron update rule becomes:\n",
    "\n",
    "$$\\alpha_i = \\alpha_i + \\eta y_i,$$\n",
    "\n",
    "and you can always retrieve the weight vector using its expansion in terms of the $\\alpha_i$.\n",
    "\n",
    "Now we're ready for the adatron - the only difference is in the initialization and update equation.\n",
    "\n",
    "Initialization:\n",
    "\n",
    "$\\alpha_i = 1$ for $i = 1,\\ldots,N$\n",
    "\n",
    "Like in the perceptron we run the algorithm until convergence, or until a fixed number of epochs has passed (an epoch is a loop over all the training data), and an epoch of training consists of the following procedure:\n",
    "\n",
    "for each training example $i=1,\\ldots,N$ perform the following steps:\n",
    "\n",
    "1.  $\\gamma = y_i * \\mathbf{w}^{t} \\mathbf{x}_i$\n",
    "2.  $\\delta\\alpha = \\eta * (1 - \\gamma)$\n",
    "3.  `if` $(\\alpha_i + \\delta\\alpha < 0)$ : $\\alpha_i = 0$, `else : ` $\\alpha_i = \\alpha_i + \\delta\\alpha$\n",
    "\n",
    "\n",
    "The variable $\\eta$ plays the role of the learning rate $\\eta$ employed in the perceptron algorithm and $\\delta \\alpha$ is the proposed magnitude of change in $\\alpha_i$. \n",
    "We note that the adatron tries to maintain a **sparse** representation in terms of the training examples by keeping many $\\alpha_i$ equal to zero.  The adatron converges to a special case of the SVM algorithm that we will learn later in the semester; this algorithm tries to maximize the margin with which each example is classified, which is captured by the variable $\\gamma$ in the algorithm (notice that the magnitude of change proposed for each $\\alpha_i$ becomes smaller as $\\gamma$ increases towards 1).\n",
    "\n",
    "**Note:** if you observe an overflow issues in running the adatron, add an upper bound on the value of $\\alpha_i$.\n",
    "\n",
    "Here's what you need to do:\n",
    "\n",
    "  - Implement the pocket algorithm and the adatron; each classifier should be implemented in a separate Python class, and use the same interface used in the code provided for the perceptron algorithm, i.e. provides the same methods with the same signature.  Make sure each classifier you use (including the original version of the perceptron) implements a bias term.\n",
    "  - Compare the performance of these variants of the perceptron on the Gisette and QSAR datasets by computing an estimate of the out of sample error on a sample of the data that you reserve for testing (the test set).  In each case reserve about 60% of the data for training, and 40% for testing.  To gain more confidence in our error estimates, repeat this experiment using 10 random splits of the data into training/test sets.  Report the average error and its standard deviation in a nicely formatted table.  Is there a version of the perceptron that appears to perform better?   (In answering this, consider the differences in performance you observe in comparison to the standard deviation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import linalg as la\n",
    "import random as rand\n",
    "from io import StringIO\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split #This packet splits the data set into training and testing \n",
    "                                                     #dataset with stratified partionong keeping the ratio of both \n",
    "                                                     #the dataset constant \n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QSAR Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QSARDataExtract():\n",
    "    dataArray = pd.read_csv(\"biodeg.csv\", delimiter=\";\", header=None)\n",
    "    Data_Y = dataArray.iloc[0:1055, 41]                    #labels\n",
    "    Data_X = dataArray.iloc[0:1055, 0:41]                  #features\n",
    "    # make col 41 values numerical\n",
    "    Data_Y = np.where(Data_Y == \"RB\", 1, -1)\n",
    "    return Data_X, Data_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gissette Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GisetteDataExtract():\n",
    "    train_X=np.genfromtxt(\"gisette_train.data\", delimiter=\" \", comments=\"#\")\n",
    "    train_Y=np.genfromtxt(\"gisette_train.labels\", delimiter=\" \", comments=\"#\")\n",
    "    valid_X=np.genfromtxt(\"gisette_valid.data\", delimiter=\" \", comments=\"#\")\n",
    "    valid_Y=np.genfromtxt(\"gisette_valid.labels\", delimiter=\" \", comments=\"#\") \n",
    "    \n",
    "    train_data_label = np.append(train_X, train_Y[:,None], axis=1)\n",
    "    valid_data_label = np.append(valid_X, valid_Y[:,None], axis=1)\n",
    "    \n",
    "    dataFrame = np.concatenate((train_data_label, valid_data_label), axis=0)\n",
    "    return dataFrame\n",
    "\n",
    "def gisette_data_split(dataFrame, shuffle = True):\n",
    "    #dataFrame = GisetteDataExtract()\n",
    "    data_Y = dataFrame[:, 5000]                        #labels\n",
    "    data_X = dataFrame[0:7000, 0:5000]                 #features\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(data_X, data_Y, test_size=0.4, shuffle = shuffle, stratify = data_Y)\n",
    "    return train_X, train_Y, test_X, test_Y, data_X, data_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron :\n",
    " \n",
    "    def __init__(self, max_iterations=1000, learning_rate=0.2) :\n",
    "        self.max_iterations = max_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "# Data training without biased   \n",
    "    def train_unbias(self, X, y) :\n",
    "        self.w = np.zeros(len(X[0]))\n",
    "        converged = False\n",
    "        iterations = 0\n",
    "        while (not converged and iterations < self.max_iterations) :\n",
    "            converged = True\n",
    "            for i in range(len(X)) :\n",
    "                if y[i] * self.decision_function(X[i]) <= 0 :\n",
    "                    self.w = self.w + y[i] * self.learning_rate * X[i]\n",
    "                    converged = False\n",
    "            iterations += 1\n",
    "        self.converged = converged\n",
    "\n",
    "# Data training with biased             \n",
    "    def train_bias(self, X, y) :\n",
    "        X = np.hstack((np.ones((len(X),1)),X))       #Adding bias term\n",
    "        self.w = np.zeros(len(X[0]))                #Initialize weight to zero \n",
    "        converged = False\n",
    "        iterations = 0\n",
    "        count = 0\n",
    "        while (not converged and iterations < self.max_iterations) :\n",
    "            converged = True\n",
    "            for i in range(len(X)) :\n",
    "                if y[i] * (self.decision_function(X[i])) <= 0 :\n",
    "                    self.w = self.w + y[i] * self.learning_rate * X[i]\n",
    "                    converged = False      \n",
    "            iterations += 1\n",
    "        self.converged = converged\n",
    "      \n",
    "    \n",
    "#   Data testing for unbiased           \n",
    "    def test_unbias(self,X,y):\n",
    "        count = 0\n",
    "        print (self.w)\n",
    "        for i in range (len(X)):\n",
    "            if np.sign(self.decision_function(X[i])) == y[i]:\n",
    "                count += 1\n",
    "        accuracy = float(count) / len(X)\n",
    "        Eout = 1 - accuracy     \n",
    "        return(Eout)\n",
    "\n",
    "# Data testing for bias\n",
    "    def test_bias(self,X,y):\n",
    "        X = np.hstack((np.ones((len(X),1)),X))\n",
    "        count = 0\n",
    "        print (self.w)\n",
    "        for i in range (len(X)):\n",
    "            if np.sign(self.decision_function(X[i])) == y[i]:\n",
    "                count += 1\n",
    "        accuracy = float(count) / len(X)\n",
    "        Eout = 1 - accuracy         \n",
    "        return(Eout)\n",
    "              \n",
    "    def decision_function(self, X) :\n",
    "        return np.inner(self.w, X)\n",
    "             \n",
    "    def predict(self, X) :     \n",
    "        scores = np.inner(self.w, X)\n",
    "        return np.sign(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pocket Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pocket :\n",
    " \n",
    "    def __init__(self, max_iterations=1000, learning_rate=0.2) :\n",
    " \n",
    "        self.max_iterations = max_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "#Data training \n",
    "    def train_pocket(self, X, y) :\n",
    "        X = np.hstack((np.ones((len(X),1)),X))                      #Adding bias term\n",
    "        self.w = np.zeros(len(X[0]))                                #Initialize weight to zero\n",
    "        self.pocketW = self.w                                       #Copy the current weight to pocket weigth \n",
    "        converged = False\n",
    "        iterations = 0\n",
    "        while (not converged and iterations < self.max_iterations) :\n",
    "            converged = True\n",
    "            for i in range(len(X)) :\n",
    "                if y[i] * (self.decision_function(X[i])) <= 0 :\n",
    "                    self.w = self.w + y[i] * self.learning_rate * X[i]\n",
    "                    converged = False\n",
    "                    if (self.error_in(self.pocketW, X, y) > self.error_in(self.w, X, y)): #update weight if the current weight gives better result that the pocket weight \n",
    "                        self.pocketW = self.w\n",
    "            iterations += 1\n",
    "        self.converged = converged\n",
    "        \n",
    "        print ('pocket W: ')\n",
    "        print (self.pocketW)\n",
    "        print ('Weight')\n",
    "        print (self.w)\n",
    "       \n",
    "\n",
    " # Data testing\n",
    "    def test_pocket(self,X,y):\n",
    "        X = np.hstack((np.ones((len(X),1)),X))\n",
    "        Eout = (np.sum(np.sign(np.inner(self.pocketW, X)) != y))/len(y)         \n",
    "        print (\"Eout: %f\" %Eout)\n",
    "        return(Eout)\n",
    "\n",
    " #In sample error calculation\n",
    "    def error_in(self,w,X,y):\n",
    "        scores = np.inner(w,X)\n",
    "        error = (np.sum(np.sign(scores)!=y))/len(y)\n",
    "        return error\n",
    " \n",
    "    def decision_function(self, x) :\n",
    "        return np.inner(self.w, x)\n",
    "             \n",
    "    def predict(self, X) :     \n",
    "        bias = np.ones((np.size(X, 0), 1)) \n",
    "        X = np.append(bias, X, 1)         \n",
    "        scores = np.inner(self.pocketW, X) \n",
    "        return np.sign(scores) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adatron Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adatron :\n",
    "    \n",
    "    def __init__(self, max_iterations=1000, learning_rate=0.2) :\n",
    " \n",
    "        self.max_iterations = max_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "# Data training\n",
    "    def  train_adatron(self, X, y):\n",
    "        bias = np.ones((np.size(X, 0), 1))       # Adding bias\n",
    "        X = np.append(bias, X, 1)   \n",
    "        self.alpha = np.ones((np.size(X, 0))) # initializing a Nx1 vector of alphas\n",
    "        \n",
    "        iterations = 0\n",
    "        m = np.zeros((len(X[0]), len(X))) \n",
    "        for i in range(len(X)):\n",
    "            m[: , i] = self.alpha[i]*y[i]*X[i]   \n",
    "        self.w = m@np.ones((np.size(X, 0), 1)) \n",
    "        self.w = self.w.reshape(len(X[0]),) \n",
    "        \n",
    "        while (iterations <= self.max_iterations):         \n",
    "            for i in range(len(X)):            \n",
    "                gamma = y[i] * self.decision_function(X[i])        # calculate gamma\n",
    "                delta_alpha = self.learning_rate * (1-gamma)     #  calculate deviation in alpha\n",
    "                self.w = self.w - (self.alpha[i] * y[i] * X[i])               \n",
    "                if (self.alpha[i] + delta_alpha < 0):            # update alpha\n",
    "                    self.alpha[i] = 0\n",
    "                else:\n",
    "                    self.alpha[i] = self.alpha[i] + delta_alpha                \n",
    "                if (self.alpha[i] > 5000): \n",
    "                    self.alpha[i] = 5000       \n",
    "                self.w = self.w + (self.alpha[i] * y[i] * X[i])            \n",
    "            iterations += 1\n",
    "        \n",
    "\n",
    "# Data testing\n",
    "    def test_adatron(self,X,y):\n",
    "        X = np.hstack((np.ones((len(X),1)),X))\n",
    "        count = 0\n",
    "        print (self.w)\n",
    "        for i in range (len(X)):\n",
    "            if np.sign(self.decision_function(X[i])) == y[i]:\n",
    "                count += 1\n",
    "        accuracy = float(count) / len(X)\n",
    "        Eout = 1 - accuracy         \n",
    "        print (\"Eout: %f\" %Eout)\n",
    "        return(Eout)\n",
    "         \n",
    "     \n",
    "    def decision_function(self, x) :\n",
    "        return np.inner(self.w, x)\n",
    "             \n",
    "    def predict(self, X) :    \n",
    "        bias = np.ones((np.size(X, 0), 1)) \n",
    "        X = np.append(bias, X, 1)        \n",
    "        scores = np.inner(self.w, X) \n",
    "        return np.sign(scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Perceptron Gisette------------\n",
      "[ 5.55111512e-17 -2.83800000e+02 -3.56000000e+01 ...  1.37200000e+02\n",
      " -1.49100000e+03  9.74200000e+02]\n",
      "[-5.55111512e-17 -5.69000000e+02 -2.89800000e+02 ... -3.41000000e+02\n",
      " -8.17200000e+02  4.23800000e+02]\n",
      "[   0.8 -230.2  320.2 ... -728.2  258.4   46.4]\n",
      "[-2.0000e-01 -1.1954e+03  6.0360e+02 ... -3.9380e+02 -1.0456e+03\n",
      " -9.2260e+02]\n",
      "[-4.0000e-01  5.7620e+02  4.9000e+01 ... -2.5200e+01 -1.5864e+03\n",
      "  1.0790e+03]\n",
      "[ 2.0000e-01  1.0896e+03  2.8200e+02 ... -5.2420e+02 -2.7080e+02\n",
      "  5.6780e+02]\n",
      "[ 2.0000e-01  5.4360e+02  8.9540e+02 ... -1.0904e+03  1.6296e+03\n",
      "  8.2340e+02]\n",
      "[-5.55111512e-17 -6.12200000e+02  3.43600000e+02 ... -1.34500000e+03\n",
      " -6.33200000e+02  7.05800000e+02]\n",
      "[-4.000e-01  1.580e+01 -3.104e+02 ...  3.320e+01 -9.230e+02 -8.508e+02]\n",
      "[ 2.000e-01  2.354e+02 -4.420e+02 ... -1.068e+02  5.480e+02 -9.594e+02]\n",
      "---------Perceptron Biased(Gissette)----------\n",
      "List of Eout is:\n",
      " [0.04107142857142854, 0.0346428571428572, 0.03214285714285714, 0.0357142857142857, 0.03035714285714286, 0.033571428571428585, 0.037142857142857144, 0.03964285714285709, 0.033571428571428585, 0.03785714285714281]\n",
      "Mean of all Eout is: 0.035571428571428566\n",
      "Standard Deviation of all Eout is: 0.003199170810936937\n",
      "----------Perceptron QSAR------------\n",
      "[ 9.00000000e+00 -1.11587600e+02 -1.13554880e+02 -9.68000000e+01\n",
      "  1.10000000e+02  7.00000000e+00 -1.78800000e+02 -5.72000000e+01\n",
      "  7.30000000e+00 -1.18000000e+01  4.80000000e+00  2.04000000e+01\n",
      " -3.62680000e+00 -2.40220400e+02  4.39726000e+01  1.70000400e+02\n",
      "  2.24487096e-13 -5.55160000e+00  8.40560000e+00 -2.78400000e+02\n",
      " -1.92000000e+02 -2.20000000e+00  3.42562000e+01 -3.24000000e+01\n",
      "  7.92000000e+01 -1.32000000e+02  2.50200000e+02 -8.84400000e+00\n",
      " -1.15238200e+02 -1.20400000e+02  1.09382000e+01  1.40068000e+01\n",
      " -2.27600000e+02 -6.62000000e+01 -4.10000000e+01  5.64000000e+01\n",
      "  6.39562000e+01  7.58896000e+01 -1.38400000e+02 -8.47194000e+01\n",
      "  4.90000000e+01  3.26000000e+01]\n",
      "[-4.980000e+01 -2.269232e+02 -2.983520e+01 -6.580000e+01  1.368000e+02\n",
      " -8.000000e+00 -2.436000e+02 -1.048000e+02  1.720000e+01 -3.200000e+00\n",
      "  3.800000e+00  1.240000e+01  1.017742e+02 -4.945800e+00  1.117572e+02\n",
      "  6.275400e+01  3.800000e+00 -3.349240e+01 -6.325060e+01 -1.946000e+02\n",
      " -2.740000e+02 -2.000000e-01 -1.123880e+02 -2.880000e+01  2.460000e+01\n",
      "  9.200000e+00  9.580000e+01  3.579200e+00 -1.127784e+02  2.460000e+01\n",
      " -1.048000e-01 -1.336940e+01 -6.240000e+01 -2.540000e+01 -7.260000e+01\n",
      "  4.200000e+01  6.563600e+00  2.755152e+02 -7.740000e+01 -5.555360e+01\n",
      "  1.786000e+02 -2.284000e+02]\n",
      "[   4.4     -145.3804   -93.41374 -219.6      -28.8        5.\n",
      " -382.2     -112.        13.02      18.8       53.        14.8\n",
      "   72.135   -122.9168    24.3282    97.5614    -9.2       12.3044\n",
      "    2.8938   -19.      -175.2       -1.4        7.9518   -13.8\n",
      "   71.2     -150.4        6.6      -35.5348  -122.974      7.8\n",
      "    6.6032     8.1528  -226.        19.4      -73.6       44.2\n",
      "    7.9156   124.3162  -105.8       -1.582    190.8       16.6    ]\n",
      "[ -53.       -78.122    -98.68658 -153.2      152.8      -57.8\n",
      " -301.2     -176.8       17.84      35.4       10.6       47.\n",
      "   78.9062  -236.5994   108.9788   103.192    -12.       -16.9264\n",
      "  -67.4314   -51.      -267.       -19.       -65.705    -26.6\n",
      "  -18.6     -109.        78.6       63.8696  -168.6044   -17.4\n",
      "   11.6374    11.9976  -159.8       77.       -47.4       38.8\n",
      "  212.6638   248.4252   -79.6     -206.5998   341.         4.6    ]\n",
      "[   2.8     -51.5848 -105.0083  -65.4     -84.6     -12.6    -179.8\n",
      " -117.4      10.38      2.       -4.6       8.6      32.576  -207.4438\n",
      "   57.8332   68.3418   -1.6       3.6038    1.9398  -40.6    -241.4\n",
      "   -2.6     -44.393   -78.      222.8     -52.2     126.6      85.4088\n",
      "  -20.0058   23.6       1.7082   30.6578 -126.8       8.6     -62.6\n",
      "   74.8      70.3384  126.2556  -67.8     -89.661   300.2     -46.2   ]\n",
      "[ -64.4       15.1192  -248.33208  -39.8     -132.        -2.2\n",
      " -271.6     -222.8       13.38      25.8       24.2      -42.8\n",
      "    4.7998  -197.962    118.09      77.6332    -5.2      -40.322\n",
      "  -87.4068  -131.2     -207.6       -2.         0.4882   -40.\n",
      "   11.2     -136.        73.       -53.7332   -21.2298   -77.6\n",
      "    6.7158    10.137   -232.8       96.2      -38.4       -0.8\n",
      "   47.4246   261.8408   -57.4      -63.349    148.4     -128.     ]\n",
      "[  37.4      -55.3708   -74.18884  -96.8      202.8       -3.2\n",
      " -314.2     -120.8       18.44       5.6       51.2        2.4\n",
      "   77.8042  -143.3994   112.7862    13.5774    -1.6       54.3682\n",
      "   34.9822  -194.4     -199.6        0.        -8.4876    -9.6\n",
      "   50.8     -111.6       10.6     -119.1662   -67.5992  -124.2\n",
      "   -0.6186     5.4168  -157.2      -65.         0.6       43.\n",
      "   16.9494   197.8292  -113.4      -42.4322   149.4      -88.8    ]\n",
      "[ -60.4     -170.746   -105.77772  -84.8     -143.8      -58.4\n",
      " -349.      -206.4       19.5       65.8       14.        57.2\n",
      "   48.0506  -109.7968    49.5276    57.5578   -18.       -25.971\n",
      "  -80.972   -138.4     -258.6       -6.4       83.196     26.\n",
      "   84.      -194.8      127.6      -80.3378     4.7986   -26.\n",
      "   14.8836     4.8228  -202.        22.2      -70.6      127.\n",
      "  -24.1448   199.8672  -196.         8.8152   171.       -78.4    ]\n",
      "[ -61.4     -64.2746  -80.1984  -26.6     247.2     -45.     -345.2\n",
      " -168.4      11.36     33.6       1.4      24.       29.3116 -103.0026\n",
      "  128.4562   41.3966  -15.      -56.0388  -73.3836  -31.8    -160.4\n",
      "   -0.4     -22.4172   51.4     146.8    -145.      139.       62.1526\n",
      " -124.5734    8.6       5.0292   -5.0458 -212.6      28.8     -49.\n",
      "   64.4      49.0288  195.0488 -167.8     -84.4244  163.2    -133.    ]\n",
      "[ -34.      -169.8286   -66.85468 -133.8      133.2      -48.4\n",
      " -224.4     -163.2       16.82      36.        63.4       18.6\n",
      "   16.716   -236.534    113.55      87.568    -26.6       -6.4276\n",
      "  -48.6194  -224.8     -337.8       -1.         2.2574    13.6\n",
      "   92.2      -87.2      195.8       -5.5296   -31.214     32.\n",
      "   -2.6414     6.9454  -190.6       65.       -67.2       21.8\n",
      "   36.911    184.9154   -61.6      -42.6458   218.4      -18.6    ]\n",
      "---------Perceptron Biased(QSAR)----------\n",
      "List of Eout is:\n",
      " [0.1255924170616114, 0.14218009478672988, 0.27725118483412325, 0.1492890995260664, 0.22511848341232232, 0.1398104265402843, 0.15402843601895733, 0.15402843601895733, 0.16113744075829384, 0.17061611374407581]\n",
      "Mean of all Eout is: 0.16990521327014219\n",
      "Standard Deviation of all Eout is: 0.04383630068087585\n",
      "----------Pocket Gisette------------\n",
      "pocket W: \n",
      "[ 5.55111512e-17 -1.27700000e+03  3.16000000e+02 ... -4.83800000e+02\n",
      " -2.64700000e+03  5.23400000e+02]\n",
      "Weight\n",
      "[ 5.55111512e-17 -1.27700000e+03  3.16000000e+02 ... -4.83800000e+02\n",
      " -2.64700000e+03  5.23400000e+02]\n",
      "Eout: 0.035000\n",
      "pocket W: \n",
      "[ 2.000e-01 -1.038e+02  9.360e+01 ... -7.082e+02  6.016e+02  2.080e+01]\n",
      "Weight\n",
      "[ 2.000e-01 -1.038e+02  9.360e+01 ... -7.082e+02  6.016e+02  2.080e+01]\n",
      "Eout: 0.030000\n",
      "pocket W: \n",
      "[ 5.55111512e-17 -3.97180000e+03  3.63600000e+02 ... -8.27600000e+02\n",
      "  1.11740000e+03  5.95400000e+02]\n",
      "Weight\n",
      "[ 5.55111512e-17 -3.97180000e+03  3.63600000e+02 ... -8.27600000e+02\n",
      "  1.11740000e+03  5.95400000e+02]\n",
      "Eout: 0.034643\n",
      "pocket W: \n",
      "[-4.0000e-01 -1.3340e+02 -3.1760e+02 ... -5.6840e+02 -1.1578e+03\n",
      "  5.9000e+01]\n",
      "Weight\n",
      "[-4.0000e-01 -1.3340e+02 -3.1760e+02 ... -5.6840e+02 -1.1578e+03\n",
      "  5.9000e+01]\n",
      "Eout: 0.035000\n",
      "pocket W: \n",
      "[ 2.0000e-01 -5.9160e+02  5.8020e+02 ... -6.1120e+02  1.3340e+02\n",
      " -1.5704e+03]\n",
      "Weight\n",
      "[ 2.0000e-01 -5.9160e+02  5.8020e+02 ... -6.1120e+02  1.3340e+02\n",
      " -1.5704e+03]\n",
      "Eout: 0.030000\n",
      "pocket W: \n",
      "[ 2.0000e-01 -2.0282e+03 -2.0320e+02 ... -7.3140e+02 -3.0850e+03\n",
      " -1.4834e+03]\n",
      "Weight\n",
      "[ 2.0000e-01 -2.0282e+03 -2.0320e+02 ... -7.3140e+02 -3.0850e+03\n",
      " -1.4834e+03]\n",
      "Eout: 0.030000\n",
      "pocket W: \n",
      "[-5.55111512e-17 -1.29820000e+03  5.38200000e+02 ... -1.58400000e+02\n",
      "  9.95600000e+02  3.52200000e+02]\n",
      "Weight\n",
      "[-5.55111512e-17 -1.29820000e+03  5.38200000e+02 ... -1.58400000e+02\n",
      "  9.95600000e+02  3.52200000e+02]\n",
      "Eout: 0.034643\n",
      "pocket W: \n",
      "[-2.0000e-01 -8.9880e+02  2.0640e+02 ... -1.3520e+02 -2.6740e+02\n",
      "  1.4668e+03]\n",
      "Weight\n",
      "[-2.0000e-01 -8.9880e+02  2.0640e+02 ... -1.3520e+02 -2.6740e+02\n",
      "  1.4668e+03]\n",
      "Eout: 0.028214\n",
      "pocket W: \n",
      "[ 2.0000e-01  2.2000e+00  4.1040e+02 ... -8.2160e+02 -8.2860e+02\n",
      " -1.0722e+03]\n",
      "Weight\n",
      "[ 2.0000e-01  2.2000e+00  4.1040e+02 ... -8.2160e+02 -8.2860e+02\n",
      " -1.0722e+03]\n",
      "Eout: 0.033571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pocket W: \n",
      "[-4.0000e-01 -9.2660e+02  8.7340e+02 ... -9.5840e+02 -1.4048e+03\n",
      "  7.7940e+02]\n",
      "Weight\n",
      "[-4.0000e-01 -9.2660e+02  8.7340e+02 ... -9.5840e+02 -1.4048e+03\n",
      "  7.7940e+02]\n",
      "Eout: 0.031786\n",
      "---------Pocket----------\n",
      "List of Eout is:\n",
      " [0.035, 0.03, 0.03464285714285714, 0.035, 0.03, 0.03, 0.03464285714285714, 0.028214285714285713, 0.03357142857142857, 0.031785714285714285]\n",
      "Mean of all Eout is: 0.03228571428571429\n",
      "Standard Deviation of all Eout is: 0.002449489742783179\n",
      "----------Pocket QSAR------------\n",
      "pocket W: \n",
      "[-7.000000e+00 -3.467900e+01 -8.850402e+01 -6.080000e+01 -2.460000e+01\n",
      " -2.320000e+01 -2.040000e+02 -1.230000e+02  1.062000e+01  1.780000e+01\n",
      " -3.400000e+01  4.600000e+00  5.971000e+00 -1.650046e+02  1.066968e+02\n",
      "  4.714940e+01  1.800000e+00  2.588000e-01 -1.372120e+01 -1.960000e+01\n",
      " -2.544000e+02 -2.200000e+00  6.242200e+00  2.000000e-01  1.430000e+02\n",
      " -7.800000e+01  4.700000e+01 -3.604400e+01 -6.939340e+01  5.040000e+01\n",
      "  1.085240e+01 -1.382600e+00 -1.706000e+02  4.600000e+01 -1.000000e+01\n",
      "  4.520000e+01 -1.678260e+01  1.380874e+02 -1.608000e+02 -1.393960e+01\n",
      "  2.186000e+02 -7.860000e+01]\n",
      "Weight\n",
      "[ -27.4      -46.8274  -100.12164  -43.8      -35.4      -31.4\n",
      " -230.4     -133.        15.3       21.       -36.         9.\n",
      "    8.9344  -225.1858   128.7922    84.0002     2.2      -14.8262\n",
      "  -40.811    -30.4     -316.        -2.2       -0.4498     6.4\n",
      "  195.6      -85.        86.4      -57.7318  -100.5826    43.\n",
      "    2.8076    -2.6138  -188.6       54.       -15.8       49.4\n",
      "   -4.6378   150.5926  -183.       -26.3738   234.6      -71.2    ]\n",
      "Eout: 0.137441\n",
      "pocket W: \n",
      "[   7.6     -14.2844  -88.1933  -53.8     -60.4     -48.6     -99.\n",
      " -110.4      20.74     -0.4      52.8      18.6      34.6224 -109.4404\n",
      "   77.2584   -2.566   -31.6      12.093     8.5056  -60.      -55.\n",
      "   -2.2      -7.449   -46.2     -10.8     -54.4      29.4      -2.8482\n",
      "   -7.503     2.8       8.2288   11.897  -118.      -12.      -42.6\n",
      "   77.6      44.25     98.8522  -66.2     -62.0816  173.      -52.    ]\n",
      "Weight\n",
      "[  16.6       -4.8564  -119.33584    1.8     -117.       -44.8\n",
      " -201.      -149.2       22.16      13.2       74.2        8.6\n",
      "   61.0816  -307.2494   126.166     53.703    -28.8       28.9982\n",
      "   19.4772  -116.6     -145.2       -8.4      -36.0608   -31.4\n",
      "  -12.4     -229.2      205.4        3.992    -90.3426    17.4\n",
      "   15.6694    18.2602  -189.8       -4.4      -68.       112.2\n",
      "  145.9958   112.6664   -50.6     -153.4876   218.6      -17.4    ]\n",
      "Eout: 0.139810\n",
      "pocket W: \n",
      "[   7.       -27.029    -84.80504  -80.2       12.       -29.6\n",
      " -151.2      -93.2       13.28     -13.6       66.4        3.4\n",
      "   42.7312   -22.3988    37.124     25.4986   -16.4        6.5776\n",
      "    8.4606   -17.2      -61.8       -0.8        3.6122   -72.\n",
      "   10.2      -37.6        0.4       -7.2332     6.5906    24.6\n",
      "   -3.8656    16.509    -63.2      -63.4      -44.6       42.\n",
      "    0.422     31.9196   -49.8      -29.4644   114.4      -76.     ]\n",
      "Weight\n",
      "[ -54.2      -55.0312   -91.05008  -48.8      150.       -18.2\n",
      " -317.      -129.8        8.58      11.6       34.8       10.4\n",
      "   43.6064   -51.9108    22.3712   109.9778   -15.6      -37.882\n",
      "  -63.4958  -140.6     -207.4       -0.8      -29.4082   -40.8\n",
      "  113.6      -89.2       76.2      -69.2384    27.8566   174.4\n",
      "    6.2612    21.5352  -102.2      -46.4      -45.        57.2\n",
      "   33.7062   103.589   -104.2      -88.5646   200.6      -80.     ]\n",
      "Eout: 0.156398\n",
      "pocket W: \n",
      "[   1.6     -135.1112   -98.65734 -141.2      114.8      -31.2\n",
      " -180.8     -108.8       15.76      17.6       44.        12.2\n",
      "   -9.9058  -118.993     47.3316    97.1174   -16.        10.539\n",
      "    4.9108  -193.8     -187.2       -0.8      -26.9066   -27.8\n",
      "    6.4     -140.2      253.8      -33.839    -25.9216    67.6\n",
      "   10.617      4.856   -129.6        2.8      -54.8       57.8\n",
      "   14.9232    70.3514   -60.2      -47.7412   247.        50.     ]\n",
      "Weight\n",
      "[   1.4     -133.993    -98.53766 -142.4      117.6      -33.8\n",
      " -181.6     -109.8       13.48      21.4       48.4        8.2\n",
      "  -12.543   -118.4376    50.206    101.8102   -12.8       10.4466\n",
      "    4.725   -195.2     -187.8       -0.8      -27.5662   -26.6\n",
      "    6.6     -141.4      256.       -33.5198   -26.5416    67.8\n",
      "   15.6304     6.2852  -131.         2.2      -53.4       63.2\n",
      "   15.846     70.2588   -59.6      -46.8514   247.6       50.     ]\n",
      "Eout: 0.158768\n",
      "pocket W: \n",
      "[   9.      -15.1806 -137.2019  -84.4     119.4      11.4    -274.8\n",
      " -128.6      19.28    -10.       66.4      -2.8      54.6748 -139.7196\n",
      "  146.5928   14.4544  -13.2      33.4146    8.653   -51.     -192.\n",
      "   -1.8     -26.1148  -89.4      76.8     -59.6     211.6      72.1298\n",
      "  -55.935   -40.        7.3296  -22.8288 -140.8     -29.6     -49.8\n",
      "   48.8      26.9614  117.1578  -66.2     -52.9446  196.       -4.2   ]\n",
      "Weight\n",
      "[   1.       -26.773   -142.80154  -81.4      154.4       11.4\n",
      " -331.6     -144.        19.86      -4.2       64.8        4.4\n",
      "   52.5952  -179.2136   138.5246    36.5152    -5.4       40.0878\n",
      "   -1.3358   -83.8     -219.4       -1.8      -51.5138   -99.4\n",
      "  132.       -71.6      272.6      110.9916   -70.3398   -60.\n",
      "    1.6902   -15.9074  -138.6      -26.6      -54.4       45.8\n",
      "   38.059    130.5762   -67.2      -71.2894   176.2       20.2    ]\n",
      "Eout: 0.127962\n",
      "pocket W: \n",
      "[   7.2     -60.5488  -78.2209 -131.8      69.8     -17.2    -252.6\n",
      " -170.8      18.76     33.4      36.6      19.       22.4984  -79.5242\n",
      "  148.5878    6.1872  -27.6      14.5012   11.8138 -117.8    -217.6\n",
      "   -0.4     -11.9406  -10.8      79.     -103.4      54.8     -12.0908\n",
      "  -44.6288    1.       13.0316  -26.875  -239.8      93.8     -82.2\n",
      "   68.8      34.026   154.7316 -112.2     -58.6202  188.6     -40.6   ]\n",
      "Weight\n",
      "[ -18.2      -64.8426   -93.38248 -140.       170.6       -7.4\n",
      " -361.4     -185.6       21.1       52.        32.         5.2\n",
      "   25.926    -98.177    130.1316    41.9522   -23.8       -4.1448\n",
      "  -15.8548  -218.6     -340.2       -0.4      -36.8618   -21.2\n",
      "  155.8     -134.4      115.4      -11.5346   -57.1628   -23.2\n",
      "    6.8736   -26.7924  -278.4       90.4      -78.        65.\n",
      "   65.4576   182.0726  -124.4     -105.787    190.       -29.4    ]\n",
      "Eout: 0.127962\n",
      "pocket W: \n",
      "[   8.4      -36.2424   -51.62774 -148.       -50.6       -8.8\n",
      " -214.6     -144.        11.9      -18.8       -9.        -2.8\n",
      "   86.4108  -137.0798   118.5982    -4.766     -5.2       36.8606\n",
      "   15.5086   -43.      -373.4      -13.2       16.2552   -11.\n",
      "  -19.8     -208.8       82.8       22.7092  -119.42     -80.4\n",
      "    5.669      3.8408  -150.6       95.8      -32.6       62.2\n",
      "   64.9618   191.7922  -166.4      -53.4898   334.        28.     ]\n",
      "Weight\n",
      "[   8.       -35.1496   -58.48946 -147.2      -51.        -8.\n",
      " -223.      -143.4       29.02     -24.2       -8.4       -2.8\n",
      "   85.6636  -143.398    119.6144    -0.492     -4.8       38.9006\n",
      "   15.4558   -43.4     -386.       -20.8       18.148     -3.6\n",
      "  -16.4     -211.4       93.6       24.3396  -127.4198   -81.8\n",
      "   13.8716     6.4     -149.       104.       -33.2       63.2\n",
      "   65.3172   195.0192  -171.8      -58.8936   335.4       27.     ]\n",
      "Eout: 0.163507\n",
      "pocket W: \n",
      "[ -22.         1.0902   -83.60284  -62.        45.4        3.4\n",
      " -235.4     -206.4       20.14      17.8       29.6       -4.2\n",
      "   -0.4182  -151.4468    37.8266    50.2364   -15.       -21.264\n",
      "  -24.9542  -186.2     -237.2       -0.6        4.6952   -15.4\n",
      "   50.8     -143.4      114.8      -41.341   -130.3206    51.8\n",
      "    0.2622    14.7298  -166.8       59.6      -56.4       23.8\n",
      "   50.1152   138.25     -63.       -87.6092   237.       -25.     ]\n",
      "Weight\n",
      "[ -36.        11.2744   -91.90226  -55.8       82.         5.2\n",
      " -259.      -213.8       19.34      18.6       25.2       -7.\n",
      "   -2.1672  -186.0726    42.7374    72.4458    -9.8      -35.5048\n",
      "  -40.9054  -243.4     -255.2       -0.6        6.4696    -6.2\n",
      "   56.2     -174.6      142.4      -53.923   -147.787     57.2\n",
      "   16.937     16.17    -174.2       54.2      -49.        32.4\n",
      "   72.8008   158.0822   -73.8     -111.2162   240.8      -14.6    ]\n",
      "Eout: 0.125592\n",
      "pocket W: \n",
      "[  26.4     -60.0076 -114.5708 -103.4      34.2     -37.4    -168.4\n",
      " -154.4      12.8     -14.8      20.        3.6      11.336  -128.3096\n",
      "   68.0584   77.7568   -9.8      29.665    31.7224 -102.8    -257.\n",
      "   -1.2      -6.6778  -29.       65.2     -69.8      -0.4      -7.1786\n",
      "    7.3926   -2.        7.6614    6.9658 -219.6      75.2     -70.6\n",
      "   23.       27.847    71.163   -16.6     -50.367   163.4      15.4   ]\n",
      "Weight\n",
      "[  33.      -112.4886  -133.57088  -94.4       68.       -37.2\n",
      " -168.8     -155.6       13.       -19.         7.6       10.4\n",
      "   16.4118  -243.8286    95.5834   136.8544    -4.6       36.9588\n",
      "   35.1964  -241.4     -393.        -1.4      -15.1966   -18.2\n",
      "  151.4      -96.8       -0.4      -28.0668   -25.9204   -42.8\n",
      "    9.6472     4.3544  -257.        67.4      -68.4       26.2\n",
      "   62.2318    96.9524   -20.6      -70.6838   151.8       37.     ]\n",
      "Eout: 0.137441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pocket W: \n",
      "[  11.6      -28.7568  -111.60288  -37.6      -20.       -54.4\n",
      " -198.8     -102.        22.98       9.2       58.4        3.\n",
      "   -1.011   -105.1198    30.3962     4.782     -8.        14.2672\n",
      "   12.5438   -13.8      -58.4       -1.8       24.3152  -123.4\n",
      "  -14.4      -15.4       31.2       15.7426   -18.0088   -57.\n",
      "    5.9982     3.3682  -162.2       -0.6      -42.8       59.\n",
      "   21.3016    63.0496   -37.8      -42.2006   132.2      -62.8    ]\n",
      "Weight\n",
      "[ -31.6      -2.3184 -146.6905  -33.2     -61.2     -65.2    -325.\n",
      " -145.4      26.2       9.8      32.4      -1.8      -6.7194 -317.5514\n",
      "   52.6316   56.6698   10.2      -9.5428  -45.0822  -55.     -153.8\n",
      "   -1.8      34.9034 -126.4      13.6     -74.8     107.6      65.2594\n",
      "  -83.4852 -183.        4.5074   16.7288 -220.2       6.6     -42.2\n",
      "   91.       96.8758   85.6408  -66.2     -91.4416  126.6     -46.2   ]\n",
      "Eout: 0.144550\n",
      "---------Pocket Biased(QSAR)----------\n",
      "List of Eout is:\n",
      " [0.13744075829383887, 0.13981042654028436, 0.15639810426540285, 0.15876777251184834, 0.12796208530805686, 0.12796208530805686, 0.16350710900473933, 0.12559241706161137, 0.13744075829383887, 0.14454976303317535]\n",
      "Mean of all Eout is: 0.1419431279620853\n",
      "Standard Deviation of all Eout is: 0.012911974147701687\n",
      "----------Adatron Gisette------------\n",
      "[-1.000e+04  8.900e+05 -6.400e+06 ... -1.027e+07 -6.905e+06  3.830e+07]\n",
      "Eout: 0.068214\n",
      "[       0. 14195000. -9050000. ...  1145000. 17605000. -7370000.]\n",
      "Eout: 0.059643\n",
      "[ 5.0000e+03  1.3710e+07  6.8450e+06 ...  2.2950e+06 -6.0235e+07\n",
      "  2.3620e+07]\n",
      "Eout: 0.095714\n",
      "[ 1.5000e+04 -1.0835e+07  9.7300e+06 ... -4.7800e+06 -1.6705e+07\n",
      " -2.0840e+07]\n",
      "Eout: 0.098214\n",
      "[-5.0000e+03  1.7775e+07  5.8750e+06 ... -1.2130e+07 -1.2640e+07\n",
      " -1.6030e+07]\n",
      "Eout: 0.068571\n",
      "[        0.  -7455000.   5825000. ... -20345000. -16505000. -12240000.]\n",
      "Eout: 0.089286\n",
      "[        0. -10430000.   2615000. ... -19105000.   9530000.  -5780000.]\n",
      "Eout: 0.055714\n",
      "[ 1.0000e+04  3.3445e+07 -1.6750e+06 ...  2.0850e+06 -2.2875e+07\n",
      "  3.9900e+06]\n",
      "Eout: 0.178571\n",
      "[        0.   5820000.   1190000. ...   1415000.  -3720000. -16385000.]\n",
      "Eout: 0.052500\n",
      "[ 5.0000e+03  3.3050e+06 -1.3600e+06 ... -2.5650e+06 -3.4540e+07\n",
      "  2.0405e+07]\n",
      "Eout: 0.091786\n",
      "---------Adatron----------\n",
      "List of Eout is:\n",
      " [0.06821428571428567, 0.05964285714285711, 0.09571428571428575, 0.0982142857142857, 0.06857142857142862, 0.0892857142857143, 0.055714285714285716, 0.1785714285714286, 0.05249999999999999, 0.09178571428571425]\n",
      "Mean of all Eout is: 0.08582142857142858\n",
      "Standard Deviation of all Eout is: 0.034944580759966155\n",
      "----------Adatron QSAR------------\n",
      "[  20000.          -69485.           32719.         -265000.\n",
      "   -5000.         -290000.         -180000.         -490000.\n",
      "  102000.         -325000.          210000.         -195000.\n",
      "  -85005.          -53280.          209840.          -37680.\n",
      " -235000.           20070.           24930.               0.\n",
      "  -60000.          -25000.           -2350.         -230000.\n",
      "  -10000.          -45000.               0.           -7400.\n",
      "   -9740.           -5000.          142555.00000001  320820.\n",
      "  -65000.         -330000.         -310000.          140000.\n",
      "  -72695.          160810.         -145000.          -95320.\n",
      "   85000.         -220000.        ]\n",
      "Eout: 0.419431\n",
      "[ 3.75799374e+04 -5.03580622e+04  6.28700589e+04 -3.75000000e+05\n",
      "  0.00000000e+00 -2.54193832e+05 -1.30000000e+05 -5.24193832e+05\n",
      "  9.83005089e+04 -1.63271883e+02  2.55483021e+05 -2.97096916e+05\n",
      "  8.06618932e+03 -1.22893143e+05  1.54063996e+05  1.43331741e+04\n",
      " -1.28227789e+05  3.46391469e+04  4.61159650e+04 -5.00000000e+03\n",
      " -7.00000000e+04 -1.00000000e+04  8.52481545e+03 -3.32096916e+05\n",
      " -2.50000000e+04 -7.00000000e+04  5.00000000e+03 -1.14589044e+04\n",
      " -3.34251827e+04 -1.50000000e+04 -4.63688342e+04  1.44219601e+05\n",
      " -1.20000000e+05 -3.22096916e+05 -3.10000000e+05  2.25799374e+04\n",
      " -1.89766311e+05  2.03184615e+05 -1.87096916e+05 -2.06608503e+05\n",
      "  5.00000000e+04 -4.20000000e+05]\n",
      "Eout: 0.227488\n",
      "[ 2.00000000e+04 -1.13015000e+05 -7.11150000e+03 -3.30000000e+05\n",
      "  1.00000000e+04 -1.55000000e+05 -1.15000000e+05 -4.10000000e+05\n",
      "  2.65000000e+04 -1.80000000e+05  2.15000000e+05 -2.45000000e+05\n",
      " -2.14850000e+04 -8.00800000e+04  1.68955000e+05 -9.65850000e+04\n",
      " -2.60000000e+05  1.75250000e+04  2.58850000e+04  0.00000000e+00\n",
      " -5.00000000e+04 -5.00000000e+03 -5.40500000e+03 -2.70000000e+05\n",
      " -1.45519152e-11 -6.00000000e+04 -5.00000000e+03 -3.61400000e+04\n",
      " -1.16000000e+04  5.00000000e+03  9.69700000e+04  3.11335000e+05\n",
      " -9.00000000e+04 -2.35000000e+05 -2.55000000e+05  2.50000000e+04\n",
      " -1.18475000e+05  1.51475000e+05 -1.25000000e+05 -1.47935000e+05\n",
      "  5.00000000e+04 -2.15000000e+05]\n",
      "Eout: 0.272512\n",
      "[  30000.   -76620.    31297.5 -310000.   -10000.  -195000.  -190000.\n",
      " -365000.   207500.  -140000.   210000.  -245000.   -79925.   -89150.\n",
      "  130400.     6240.  -160000.    28125.    35990.    -5000.   -40000.\n",
      "  -25000.     8340.  -255000.   -15000.   -55000.    -5000.    -8575.\n",
      "  -21205.    -5000.    28455.   230895.   -70000.  -265000.  -225000.\n",
      "   90000.  -231445.   149345.  -105000.  -202890.    35000.  -330000. ]\n",
      "Eout: 0.556872\n",
      "[  41201.49134     -25971.92992532   22290.97213595 -270000.\n",
      "  -30000.         -343798.50866    -205000.         -425000.\n",
      " -176648.53272603  -87597.01732     211201.49134    -335000.\n",
      "  -31500.          -77925.12267644  114797.908113     50600.31830795\n",
      " -350000.           39142.88686598   48399.34934182  -20000.\n",
      "  -45000.          -10000.           22466.79722514 -275000.\n",
      "   -5000.          -85000.               0.           -2444.64400368\n",
      "   -2631.59717776   -5000.         -215285.          127186.3025159\n",
      "  -70000.         -245000.         -386395.52598      14805.96535999\n",
      " -101015.40651236  192160.54161162 -212597.01732     -74366.29931463\n",
      "   45000.         -360000.        ]\n",
      "Eout: 0.336493\n",
      "[  30000.          -48765.           41825.         -365000.\n",
      "  -20000.         -265000.         -125000.         -495000.\n",
      "  160500.          -55000.          190000.         -275000.\n",
      "  -46320.          -89635.          149885.           14625.\n",
      "  -35000.           26215.           37995.          -15000.\n",
      "  -75000.           -5000.            8990.         -245000.\n",
      "  -20000.         -105000.            5000.           -7865.\n",
      "   -9240.          -15000.           97105.00000001  271485.\n",
      "  -70000.         -345000.         -290000.          110000.\n",
      " -210345.          139995.          -90000.         -216165.\n",
      "   35000.         -270000.        ]\n",
      "Eout: 0.511848\n",
      "[  35000.   -3690.  136747. -350000.  -20000. -215000. -100000. -455000.\n",
      " -105000.  -35000.  190000. -215000.    6015.  -67960.  166595.  107750.\n",
      "  -85000.   31410.   43960.   -5000.  -50000.  -10000.   12560. -275000.\n",
      "  -25000. -105000.    5000.   11255.  -10605.  -10000.   15805.  298790.\n",
      "  -70000. -270000. -340000.   90000. -163800.  154015. -130000. -135565.\n",
      "   40000. -280000.]\n",
      "Eout: 0.319905\n",
      "[  30680.80429997  -40134.41614293    9802.8640131  -360000.\n",
      "   -5000.         -280915.17420017 -165000.         -493638.39140006\n",
      " -222459.78500142  -55000.          282723.21719989 -450915.17420017\n",
      "    5282.49983681  -61608.57952722  206021.57399949   33446.74896009\n",
      "   93169.65159966   26329.42038597   37694.33116437       0.\n",
      "  -70000.          -20000.            7683.94065666 -302957.58710008\n",
      "   -5000.          -65000.          -10000.           -5550.72378316\n",
      "   -3425.            5000.          151148.78952234  353061.52231583\n",
      "  -60000.         -300000.         -440915.17420017   20000.\n",
      "  -67753.11111381  144687.01074993 -203638.39140006  -73937.34816076\n",
      "   70000.         -390000.        ]\n",
      "Eout: 0.322275\n",
      "[  35000.   -66700.    51516.5 -330000.    10000.  -310000.  -205000.\n",
      " -420000.    39000.  -200000.   230000.  -390000.   -50350.   -86255.\n",
      "  188855.     8000.  -160000.    32915.    42875.        0.   -35000.\n",
      "  -10000.    11790.  -290000.   -10000.   -70000.   -10000.    -6295.\n",
      "  -14290.        0.    73270.   289050.  -100000.  -255000.  -330000.\n",
      "   20000.  -103595.   173515.  -130000.  -108135.   100000.  -350000. ]\n",
      "Eout: 0.168246\n",
      "[   5000.  -104820.     9165.5 -190000.   -25000.  -290000.  -190000.\n",
      " -420000.   -75500.  -170000.   195000.  -375000.   -61425.  -101045.\n",
      "   96385.  -152685.    20000.     5390.     6225.   -25000.   -45000.\n",
      "  -10000.   -13095.  -245000.   -15000.   -35000.    -5000.   -35100.\n",
      "    6445.   -15000.   102320.    43665.   -55000.  -265000.  -430000.\n",
      "  -95000.  -110895.    83820.  -195000.  -194985.    55000.  -255000. ]\n",
      "Eout: 0.336493\n",
      "---------Adatron Biased(QSAR)----------\n",
      "List of Eout is:\n",
      " [0.06821428571428567, 0.05964285714285711, 0.09571428571428575, 0.0982142857142857, 0.06857142857142862, 0.0892857142857143, 0.055714285714285716, 0.1785714285714286, 0.05249999999999999, 0.09178571428571425, 0.419431279620853, 0.2274881516587678, 0.2725118483412322, 0.5568720379146919, 0.3364928909952607, 0.5118483412322274, 0.3199052132701422, 0.3222748815165877, 0.16824644549763035, 0.3364928909952607]\n",
      "Mean of all Eout is: 0.216488913337847\n",
      "Standard Deviation of all Eout is: 0.15548456833843796\n"
     ]
    }
   ],
   "source": [
    "# main function\n",
    "\n",
    "if __name__=='__main__' :\n",
    "    p = Perceptron()\n",
    "    po = Pocket()\n",
    "    a = Adatron()\n",
    "    #-------Gisette----------\n",
    "    Biased_Eout = []\n",
    "    Biased_Eout_Pocket = []\n",
    "    Biased_Eout_Adadtron = []\n",
    "    #-------QSAR----------\n",
    "    Biased_Eout_QSAR = []\n",
    "    Biased_Eout_QSAR_Pocket =[]\n",
    "    Biased_Eout_QSAR_Adadtron = []\n",
    "    \n",
    "   \n",
    "    print(\"----------Perceptron Gisette------------\")\n",
    "    \n",
    "    dataFrame = GisetteDataExtract()\n",
    "    error = 0\n",
    "    Biased_mean = 0\n",
    "    Biased_sd = 0\n",
    "    for i in range(10):\n",
    "        train_X, train_Y, test_X, test_Y, X, Y = gisette_data_split(dataFrame)\n",
    "        p.train_bias(train_X,train_Y)\n",
    "        error = p.test_bias(test_X,test_Y)\n",
    "        Biased_Eout.append(error)\n",
    "    Biased_mean = np.mean(Biased_Eout)    \n",
    "    Biased_sd = np.std(Biased_Eout)\n",
    "    print(\"---------Perceptron Biased(Gissette)----------\")\n",
    "    print(\"List of Eout is:\\n\",Biased_Eout)\n",
    "    print(\"Mean of all Eout is:\", Biased_mean)\n",
    "    print(\"Standard Deviation of all Eout is:\", Biased_sd)\n",
    "\n",
    "    \n",
    "    print(\"----------Perceptron QSAR------------\")\n",
    "    \n",
    "    Data_X, Data_Y = QSARDataExtract()\n",
    "    error = 0 \n",
    "    Biased_mean_QSAR=0\n",
    "    Biased_sd_QSAR=0\n",
    "    for i in range(10):\n",
    "        train_X,  test_X, train_Y, test_Y = train_test_split(Data_X, Data_Y, test_size=0.4, stratify = Data_Y)\n",
    "        p.train_bias(train_X,train_Y)\n",
    "        error = p.test_bias(test_X,test_Y)\n",
    "        Biased_Eout_QSAR.append(error)\n",
    "    Biased_mean_QSAR = np.mean(Biased_Eout_QSAR)    \n",
    "    Biased_sd_QSAR = np.std(Biased_Eout_QSAR)\n",
    "    print(\"---------Perceptron Biased(QSAR)----------\")\n",
    "    print(\"List of Eout is:\\n\",Biased_Eout_QSAR)\n",
    "    print(\"Mean of all Eout is:\", Biased_mean_QSAR)\n",
    "    print(\"Standard Deviation of all Eout is:\", Biased_sd_QSAR)\n",
    "    \n",
    "    \n",
    "    print(\"----------Pocket Gisette------------\")\n",
    "    \n",
    "    dataFrame = GisetteDataExtract()\n",
    "    error = 0\n",
    "    Pocket_mean=0\n",
    "    Pocket_sd=0\n",
    "    for i in range(10):\n",
    "        train_X, train_Y, test_X, test_Y, X, Y = gisette_data_split(dataFrame)\n",
    "        po.train_pocket(train_X,train_Y)\n",
    "        error = po.test_pocket(test_X,test_Y)\n",
    "        Biased_Eout_Pocket.append(error)\n",
    "    Pocket_mean = np.mean(Biased_Eout_Pocket)    \n",
    "    Pocket_sd = np.std(Biased_Eout_Pocket)\n",
    "    print(\"---------Pocket----------\")\n",
    "    print(\"List of Eout is:\\n\",Biased_Eout_Pocket)\n",
    "    print(\"Mean of all Eout is:\", Pocket_mean)\n",
    "    print(\"Standard Deviation of all Eout is:\", Pocket_sd)\n",
    "    \n",
    "\n",
    "    print(\"----------Pocket QSAR------------\")\n",
    "    \n",
    "    Data_X, Data_Y = QSARDataExtract()\n",
    "    error = 0\n",
    "    Biased_mean_QSAR = 0\n",
    "    Biased_sd_QSAR = 0\n",
    "    for i in range(10):\n",
    "        train_X,  test_X, train_Y, test_Y = train_test_split(Data_X, Data_Y, test_size=0.4, stratify = Data_Y)\n",
    "        po.train_pocket(train_X,train_Y)\n",
    "        error = po.test_pocket(test_X,test_Y)\n",
    "        Biased_Eout_QSAR_Pocket.append(error)\n",
    "    Biased_mean_QSAR = np.mean(Biased_Eout_QSAR_Pocket)    \n",
    "    Biased_sd_QSAR = np.std(Biased_Eout_QSAR_Pocket)\n",
    "    print(\"---------Pocket Biased(QSAR)----------\")\n",
    "    print(\"List of Eout is:\\n\",Biased_Eout_QSAR_Pocket)\n",
    "    print(\"Mean of all Eout is:\", Biased_mean_QSAR)\n",
    "    print(\"Standard Deviation of all Eout is:\", Biased_sd_QSAR)\n",
    "    \n",
    "    \n",
    "     \n",
    "    print(\"----------Adatron Gisette------------\")\n",
    "    \n",
    "    dataFrame = GisetteDataExtract()\n",
    "    error = 0\n",
    "    Adatron_mean = 0\n",
    "    Adatron_sd = 0\n",
    "    for i in range(10):\n",
    "        train_X, train_Y, test_X, test_Y, X, Y = gisette_data_split(dataFrame)\n",
    "        a.train_adatron(train_X,train_Y)\n",
    "        error = a.test_adatron(test_X,test_Y)\n",
    "        Biased_Eout_Adadtron.append(error)\n",
    "    Adatron_mean = np.mean(Biased_Eout_Adadtron)    \n",
    "    Adatron_sd = np.std(Biased_Eout_Adadtron)\n",
    "    print(\"---------Adatron----------\")\n",
    "    print(\"List of Eout is:\\n\",Biased_Eout_Adadtron)\n",
    "    print(\"Mean of all Eout is:\", Adatron_mean)\n",
    "    print(\"Standard Deviation of all Eout is:\", Adatron_sd)\n",
    "    \n",
    "    \n",
    "    print(\"----------Adatron QSAR------------\")\n",
    "    \n",
    "    Data_X, Data_Y = QSARDataExtract()\n",
    "    error = 0\n",
    "    Adatron_mean = 0\n",
    "    Adatron_sd = 0 \n",
    "    for i in range(10):\n",
    "        train_X,  test_X, train_Y, test_Y = train_test_split(Data_X, Data_Y, test_size=0.4, stratify = Data_Y)\n",
    "        a.train_adatron(train_X,train_Y)\n",
    "        error = a.test_adatron(test_X,test_Y)\n",
    "        Biased_Eout_Adadtron.append(error)\n",
    "    Adatron_mean = np.mean(Biased_Eout_Adadtron)    \n",
    "    Adatron_sd = np.std(Biased_Eout_Adadtron)\n",
    "    print(\"---------Adatron Biased(QSAR)----------\")\n",
    "    print(\"List of Eout is:\\n\",Biased_Eout_Adadtron)\n",
    "    print(\"Mean of all Eout is:\", Adatron_mean)\n",
    "    print(\"Standard Deviation of all Eout is:\", Adatron_sd)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description:\n",
    "From the above results we can say that, the perceptron algorithm updates the weight for each feature. Whereas in Pocket algoirithm, the weight for which the error is minimum, is considered. In Adatron, the weight is dependent on the contributionn parameter ($\\alpha$ - a).\n",
    "\n",
    "\n",
    "|Variants|Mean Error| Standard Deviation|\n",
    "|--|--|--|\n",
    "|Gisette-Perceptron|0.03557|0.003199|\n",
    "|QSAR-Perceptron|0.1699|0.04383|\n",
    "|Gisette-Pocket|0.032285|0.002449|\n",
    "|QSAR-Pocket|0.14194|0.01291|\n",
    "|Gisette-Adatron|0.08582|0.03494|\n",
    "|QSAR-Adatron|0.2164|0.15548|\n",
    "\n",
    "From the above results, we can conclude that the error for Gisette data set is minimum with Pocket Algorithm and maximum error is seen with Adatron Algorithm. For QSAR data set the error is minimum with Pocket Algorithm and maximum error is seen with Adatron Algorithm.\n",
    "\n",
    "This could be because the pocket algorithm considers the best possible weight for the given sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Learning Curves \n",
    "\n",
    "Whenever we train a classifier it is useful to know if we have collected a sufficient amount of data for accurate classification.  A good way of determining that is to construct a **learning curve**, which is a plot of classifier performance (i.e. its error) as a function of the number of training examples.  Plot a learning curve for the perceptron algorithm (with bias) using the Gisette dataset.  The x-axis for the plot (number of training examples) should be on a logarithmic scale - something like 10,20,40,80,200,400,800.  Use numbers that are appropriate for the dataset at hand, choosing values that illustrate the variation that you observe.  What can you conclude from the learning curve you have constructed?  Make sure that you use a fixed test set to evaluate performance while varying the size of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17142857142857143\n",
      "0.16428571428571428\n",
      "0.13968253968253969\n",
      "0.13214285714285715\n",
      "0.12857142857142856\n",
      "0.11746031746031746\n",
      "0.11360544217687076\n",
      "0.10535714285714286\n",
      "0.10264550264550265\n",
      "0.10047619047619048\n",
      "0.09956709956709957\n",
      "0.09484126984126984\n",
      "0.09194139194139195\n",
      "0.09047619047619047\n",
      "0.08984126984126985\n",
      "0.0880952380952381\n",
      "0.08739495798319327\n",
      "0.08544973544973546\n",
      "0.0837092731829574\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.000e-01  6.208e+02 -8.060e+01 ... -3.190e+02 -2.208e+02  5.405e+02]\n",
      "Accuracy: 0.968929\n",
      "Eout: 0.031071\n",
      "([17.142857142857142, 16.428571428571427, 13.968253968253968, 13.214285714285715, 12.857142857142856, 11.746031746031745, 11.360544217687076, 10.535714285714286, 10.264550264550266, 10.047619047619047, 9.956709956709958, 9.484126984126984, 9.194139194139195, 9.047619047619047, 8.984126984126984, 8.80952380952381, 8.739495798319327, 8.544973544973546, 8.37092731829574], [210.0, 420.0, 630.0, 840.0, 1050.0, 1260.0, 1470.0, 1680.0, 1890.0, 2100.0, 2310.0, 2520.0, 2730.0, 2940.0, 3150.0, 3360.0, 3570.0, 3780.0, 3990.0])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import random as rand\n",
    "from matplotlib import pyplot as plt\n",
    " \n",
    "class Perceptron_learning_curve :\n",
    " \n",
    "    def __init__(self, max_iterations=100, learning_rate=0.1) :\n",
    " \n",
    "        self.max_iterations = max_iterations\n",
    "        self.learning_rate = learning_rate\n",
    " \n",
    "    def train_bias(self, X, y) :\n",
    "        \n",
    "        X = np.hstack((np.ones((len(X),1)),X))\n",
    "        self.w = np.zeros(len(X[0]))\n",
    "        converged = False\n",
    "        iterations = 0\n",
    "        count = 0\n",
    "        Error = []\n",
    "        range_j = []        \n",
    "        j = len(X)/20      \n",
    "        while (not converged and iterations < self.max_iterations) :\n",
    "            converged = True\n",
    "            for i in range(len(X)) :\n",
    "                if y[i] * (self.decision_function(X[i])) <= 0 :\n",
    "                    self.w = self.w + y[i] * self.learning_rate * X[i]\n",
    "                    converged = False       \n",
    "                    if np.sign(self.decision_function(X[i])) == y[i]:\n",
    "                        count += 1\n",
    "                if i == j : \n",
    "                    err = (float(count) / j )\n",
    "                    accuracy = (1- err)*100\n",
    "                    print(err)\n",
    "                    Error.append(err*100)\n",
    "                    \n",
    "                    range_j.append(j)                    \n",
    "                    j += len(X)/20\n",
    "            iterations += 1\n",
    "        self.converged = converged\n",
    "        plot_data(Error,range_j)\n",
    "        return(Error, range_j)            \n",
    "                                 \n",
    "    def test_bias(self,X,y):\n",
    "        X = np.hstack((np.ones((len(X),1)),X))\n",
    "        count = 0\n",
    "        print (self.w)\n",
    "        for i in range (len(X)):\n",
    "            if np.sign(self.decision_function(X[i])) == y[i]:\n",
    "                count += 1\n",
    "        accuracy = float(count) / len(X)\n",
    "        Eout = 1 - accuracy     \n",
    "        print (\"Accuracy: %f\" %accuracy)    \n",
    "        print (\"Eout: %f\" %Eout)\n",
    "        return()\n",
    "         \n",
    "     \n",
    "    def decision_function(self, x) :\n",
    "        return np.inner(self.w, x)\n",
    "             \n",
    "    def predict(self, X) :\n",
    "         \n",
    "        scores = np.inner(self.w, X)\n",
    "        return np.sign(scores)\n",
    "         \n",
    "def plot_data(Error, range_j) :\n",
    "    plt.xscale('log')\n",
    "    plt.plot(range_j, Error)   \n",
    "    plt.xlabel(\"Number of Training Examples\")\n",
    "    plt.ylabel(\"Percentage Error\")          \n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.show() \n",
    " \n",
    "if __name__=='__main__' :\n",
    "    accuracy = []\n",
    "    p = Perceptron_learning_curve()\n",
    "    dataFrame = GisetteDataExtract()\n",
    "    train_X, train_Y, test_X, test_Y, X, Y = gisette_data_split(dataFrame)\n",
    "    accuracy = p.train_bias(train_X,train_Y)\n",
    "    p.test_bias(test_X,test_Y)\n",
    "    print (accuracy)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results  of  the  learning  curve**:\n",
    "The  learning  curve  obtained  from  Gisette  data  is  an  ever  decreasing  curve.\n",
    "It  starts  at  approximately 22%  accuracy  and  goes  up  to  about  8%.  This shows  that  the  accuracy  of  the algorithm  increases  with  the  number  of training  examples.  When  there  are  more  training  examples,  the algorithm has  more  data  to  work  with  and  the  weights  get  updated  more  accurately  which  leads  to  a better  result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Data normalization \n",
    "\n",
    "In this section we will explore the effect of normalizing the data, focusing on normalization of features.  The simplest form of normalization is to scale each feature to be in the range [-1, 1].  We'll call this **scaling**.\n",
    "\n",
    "Here's what you need to do:\n",
    "\n",
    "  - Explain how to scale the data to be in the range [-1, 1].\n",
    "  - Compare the accuracy of the perceptron with bias on the original data and the scaled version of the heart dataset.  Does one of them lead to better performance?  Explain why you think this happens.  \n",
    "  - An alternative way of normalizing the data is to **standardize** it:  for each feature subtract the mean and divide by its standard deviation.  What can you say about the range of the resulting features in this case?  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "#### 1. Scaling  the  data  to  be  in  the  range  [-1,  1].\n",
    "\n",
    "Data  can  be  normalized  such  that  scaled  data  will  be  range  of  [-1,1]  bymultiplying  the  selected  value  with  the  absolute  maximum  term  in  thesame  feature.  \n",
    "$$Normalized({X})  = \\frac{X}{abs(Max)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heart data extraction** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_scaled_data():                                #data extraction with scaling\n",
    "    data=np.genfromtxt(\"heart.data\", delimiter=\",\", comments=\"#\")\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()    \n",
    "    data = data[:,1:]\n",
    "    train_y = data[:170,:1]\n",
    "    train_X = min_max_scaler.fit_transform(data[:170,1:])\n",
    "    return train_X,train_y\n",
    "\n",
    "def test_scaled_data():                              #data testing for scaled dataset\n",
    "    data=np.genfromtxt(\"heart.data\", delimiter=\",\", comments=\"#\")\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()    \n",
    "    data = data[:,1:]\n",
    "    test_y = data[170:,:1]\n",
    "    test_X = min_max_scaler.fit_transform(data[170:,1:])\n",
    "    return test_X,test_y\n",
    "\n",
    "def train_data():                                    #data extraction without scaling\n",
    "    data=np.genfromtxt(\"heart.data\", delimiter=\",\", comments=\"#\") \n",
    "    data = data[:,1:]\n",
    "    train_y = data[:170,:1]\n",
    "    train_X = data[:170,1:]\n",
    "    return train_X,train_y\n",
    "\n",
    "def test_data():                                     #data testing for non-scaled datatset \n",
    "    data=np.genfromtxt(\"heart.data\", delimiter=\",\", comments=\"#\")\n",
    "    data = data[:,1:]\n",
    "    test_y = data[170:,:1]\n",
    "    test_X = data[170:,1:]\n",
    "    return test_X,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.4000e+00 -2.4740e+02  8.7780e+02  1.8162e+03  2.1980e+02  9.0000e+00\n",
      "  1.6920e+02  1.6572e+03 -1.8280e+02  6.9360e+02  1.7902e+03  1.1232e+03\n",
      "  3.3854e+03  2.6078e+03]\n",
      "Error without scaling 0.52\n",
      "[-1.20000000e+00 -1.00000000e+00  4.00000000e-01  4.66666667e-01\n",
      "  1.35094340e+00  8.23744292e-01 -5.55111512e-17  1.00000000e-01\n",
      " -9.14516129e-01  2.00000000e-01  8.21428571e-02  7.00000000e-01\n",
      "  2.40000000e+00  6.50000000e-01]\n",
      "Error with scaling 0.25\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__' :\n",
    "    \n",
    "    p = Perceptron()\n",
    "    error1 = 0\n",
    "    error2 = 0\n",
    "    train_X, train_Y = train_data()\n",
    "    test_X, test_Y = test_data()        \n",
    "    p.train_bias(train_X,train_Y)\n",
    "    error1 = p.test_bias(test_X,test_Y)\n",
    "    print(\"Error without scaling\", error1)\n",
    "    \n",
    "    scaled_train_X, scaled_train_Y = train_scaled_data()\n",
    "    scaled_test_X, scaled_test_Y = test_scaled_data()        \n",
    "    p.train_bias(scaled_train_X,scaled_train_Y)\n",
    "    error2 = p.test_bias(scaled_test_X,scaled_test_Y)\n",
    "    print(\"Error with scaling\", error2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Error for Scaled and Without Scaled Data:\n",
    "Scaling the data puts all the features within the set limits. This takes less number of iterations to converge to the best solutions. Hence, scaling gives us less error compared to without scaling.\n",
    "\n",
    "|Variant|Accuracy %|Error|\n",
    "|-|--|--|\n",
    "|Original Data|48|0.52|\n",
    "|Scaled Data|75|0.25|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.  Scaling  VS  Standardization.\n",
    "\n",
    "The  main  difference  between  the  2  methods  of  normalization  is  that  we  can  have a  set  boundaries  for scaled  data  where  as  the  standardized  data  is unbounded. Both  the  approaches  have  their  advantages  and  limits. Scaling can  discard  some  information  which  can  be  a  problem  sometimes  and standardization  isn’t  very  useful  when  the  minimum  and  maximum  values of  a  data  are  unclear. Standardized  can  be  a  better  idea  when  the  range  of  features  is  large  and distance  plays  a  role  in  generating  weights.  For  instance  in  Support  Vector Machine,  a  feature  with  greater  numerical  value  can  over  shadow  a  feature with  smaller  numerical  value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
